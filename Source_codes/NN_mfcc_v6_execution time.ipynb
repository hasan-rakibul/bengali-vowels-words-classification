{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers as opt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import losses\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "n_MFCC=13\n",
    "column_names= ['MFCC_'+str(i) for i in range(1,n_MFCC+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"/home/rakibul/WORK/RESEARCH/Speech_Recognition/MAIN STUDY/GitHub Repo/Vowel_Data/Vowel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "def get_mfcc(file_path):\n",
    "    wave, sr= librosa.load(file_path, mono=True, sr=None)\n",
    "    mfcc=librosa.feature.mfcc(wave,sr=sr,n_mfcc=n_MFCC)\n",
    "    mfcc_df=pd.DataFrame(mfcc.T,columns=column_names) #convert array to pandas df\n",
    "    return mfcc_df\n",
    "\n",
    "def get_all_mfcc(path=DATA_PATH):\n",
    "    labels=sorted(os.listdir(path))\n",
    "    all_mfcc=pd.DataFrame() #empty dataframe to store all_mfccs\n",
    "    \n",
    "    for label_index,label in enumerate(labels):\n",
    "        folder_mfcc=pd.DataFrame() #empty dataframe to store mfcc of indivuals vowels/words i.e. folders\n",
    "\n",
    "        speech_files=[path + label + '/' + file for file in sorted(os.listdir(path+'/'+label))]\n",
    "        for file in tqdm(speech_files,\"Reading Speech of label -'{}'\".format(label)):\n",
    "            #print(file)\n",
    "            individual_mfcc=get_mfcc(file_path=file)\n",
    "            folder_mfcc=folder_mfcc.append(individual_mfcc,ignore_index=True)\n",
    "        \n",
    "        folder_mfcc['label']=label_index  #new columns for encoding label\n",
    "        \n",
    "        all_mfcc=all_mfcc.append(folder_mfcc,ignore_index=True)\n",
    "        \n",
    "    return all_mfcc\n",
    "\n",
    "def get_train_test(split_ratio=0.8,random_state=42):\n",
    "    all_mfccs=get_all_mfcc()\n",
    "    \n",
    "    y=all_mfccs['label'].values\n",
    "    \n",
    "    only_mfcc=all_mfccs.drop(labels=['label'],axis=1)\n",
    "    standard_mfcc=(only_mfcc-only_mfcc.mean())/only_mfcc.std()\n",
    "    X=standard_mfcc.values\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "    return train_test_split(X,y,test_size=(1-split_ratio),random_state=random_state,shuffle=True,stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test=get_train_test()\n",
    "y_train_hot=to_categorical(y_train)\n",
    "y_test_hot=to_categorical(y_test)\n",
    "\n",
    "np.random.seed(seed)\n",
    "model_v=Sequential()\n",
    "model_v.add(Dense(64, activation='tanh',input_shape=(X_train.shape[1],)))\n",
    "# model_v.add(Dense(128, activation='tanh'))\n",
    "# model_v.add(Dense(64, activation='tanh'))\n",
    "model_v.add(Dense(32, activation='tanh'))\n",
    "model_v.add(Dense(16, activation='tanh'))\n",
    "model_v.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model_v.compile(\n",
    "    optimizer=opt.Adam(learning_rate=0.005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_v=model_v.fit(X_train, y_train_hot, epochs=50, batch_size=X_train.shape[0], validation_data=(X_test,y_test_hot))\n",
    "\n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.875412464141846\n"
     ]
    }
   ],
   "source": [
    "elapsed_time=end_time-start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"/home/rakibul/WORK/RESEARCH/Speech_Recognition/MAIN STUDY/GitHub Repo/Word_Data/Word/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "\n",
    "def get_mfcc(file_path):\n",
    "    wave, sr= librosa.load(file_path, mono=True, sr=None)\n",
    "    mfcc=librosa.feature.mfcc(wave,sr=sr,n_mfcc=n_MFCC)\n",
    "    mfcc_df=pd.DataFrame(mfcc.T,columns=column_names) #convert array to pandas df\n",
    "    return mfcc_df\n",
    "\n",
    "def get_all_mfcc(path=DATA_PATH):\n",
    "    labels=sorted(os.listdir(path))\n",
    "    all_mfcc=pd.DataFrame() #empty dataframe to store all_mfccs\n",
    "    \n",
    "    for label_index,label in enumerate(labels):\n",
    "        folder_mfcc=pd.DataFrame() #empty dataframe to store mfcc of indivuals vowels/words i.e. folders\n",
    "\n",
    "        speech_files=[path + label + '/' + file for file in sorted(os.listdir(path+'/'+label))]\n",
    "        for file in tqdm(speech_files,\"Reading Speech of label -'{}'\".format(label)):\n",
    "            #print(file)\n",
    "            individual_mfcc=get_mfcc(file_path=file)\n",
    "            folder_mfcc=folder_mfcc.append(individual_mfcc,ignore_index=True)\n",
    "        \n",
    "        folder_mfcc['label']=label_index  #new columns for encoding label\n",
    "        \n",
    "        all_mfcc=all_mfcc.append(folder_mfcc,ignore_index=True)\n",
    "        \n",
    "    return all_mfcc\n",
    "\n",
    "def get_train_test(split_ratio=0.8,random_state=42):\n",
    "    all_mfccs=get_all_mfcc()\n",
    "    \n",
    "    y=all_mfccs['label'].values\n",
    "    \n",
    "    only_mfcc=all_mfccs.drop(labels=['label'],axis=1)\n",
    "    standard_mfcc=(only_mfcc-only_mfcc.mean())/only_mfcc.std()\n",
    "    X=standard_mfcc.values\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "    return train_test_split(X,y,test_size=(1-split_ratio),random_state=random_state,shuffle=True,stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test=get_train_test()\n",
    "y_train_hot=to_categorical(y_train)\n",
    "y_test_hot=to_categorical(y_test)\n",
    "\n",
    "np.random.seed(seed)\n",
    "model_w=Sequential()\n",
    "model_w.add(Dense(64, activation='tanh',input_shape=(X_train.shape[1],)))\n",
    "# model_w.add(Dense(128, activation='tanh'))\n",
    "# model_w.add(Dense(64, activation='tanh'))\n",
    "model_w.add(Dense(32, activation='tanh'))\n",
    "model_w.add(Dense(16, activation='tanh'))\n",
    "model_w.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model_w.compile(\n",
    "    optimizer=opt.Adam(learning_rate=0.005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_w=model_w.fit(X_train, y_train_hot, epochs=50, batch_size=X_train.shape[0], validation_data=(X_test,y_test_hot))\n",
    "\n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2920167446136475\n"
     ]
    }
   ],
   "source": [
    "elapsed_time=end_time-start_time\n",
    "print(elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
